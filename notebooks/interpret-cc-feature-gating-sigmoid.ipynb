{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch transformers datasets nltk jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To import the Transformer Models\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset does not contain class labels, so we need to explicitly provide it\n",
    "data_path = '../data/ag-news-classification-dataset'\n",
    "train_df=pd.read_csv(os.path.join(data_path,'train.csv'),names=['label','Title','Description'])\n",
    "val_df=pd.read_csv(os.path.join(data_path,'test.csv'),names=['label','Title','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the 'title' and 'description' column\n",
    "train_df['text']=(train_df['Title']+ \" \" + train_df['Description'])\n",
    "train_df.drop(columns=['Title','Description'],axis=1,inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the 'title' and 'description' column\n",
    "val_df['text']=(val_df['Title']+val_df['Description'])\n",
    "val_df.drop(columns=['Title','Description'],axis=1,inplace=True)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text=re.sub(r'[\\\\-]',' ',text)\n",
    "    text=re.sub(r'[,.$#?;:\\'(){}!|0-9]',' ',text)\n",
    "    return text\n",
    "\n",
    "# the apply method applies a function along an axis of dataframe\n",
    "train_df['text']=train_df['text'].apply(remove_punctuations)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['text']=val_df['text'].apply(remove_punctuations)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    clean_text=[]\n",
    "    for word in text.split(' '):\n",
    "        if word not in english_stopwords:\n",
    "            clean_text.append(word)\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "# remove stopwords\n",
    "train_df['text']=train_df['text'].apply(remove_stopwords)\n",
    "\n",
    "# the class label in dataset contains labels as 1,2,3,4 but the model needs 0,1,2,3, so we subtract 1 from all\n",
    "train_df['label']=train_df['label'].apply(lambda x:x-1)\n",
    "\n",
    "# remove stopwords\n",
    "val_df['text']=val_df['text'].apply(remove_stopwords)\n",
    "\n",
    "# the class label in dataset contains labels as 1,2,3,4 but the model needs 0,1,2,3, so we subtract 1 from all\n",
    "val_df['label']=val_df['label'].apply(lambda x:x-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=train_test_split(train_df[['text','label']],train_size=.3,shuffle=True, random_state=0)\n",
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set has 36000 samples and testing set has 10000 samples for the purpose of a fast training loop\n",
    "test_df = test_df[:10000]\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "raw_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "raw_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"wesleyacheng/news-topic-classification-with-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"wesleyacheng/news-topic-classification-with-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.encoding = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.loc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            row['text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.encoding = encoding\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "            \n",
    "        labels = row['label']\n",
    "        return input_ids, torch.tensor(labels)\n",
    "    \n",
    "# Assuming train_df is your training dataframe and tokenizer is defined\n",
    "dataset = CustomDataset(train_df, tokenizer, max_length=512)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train InterpretCC Feature Gating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sigmoid(logits: torch.Tensor, tau: float = 1, hard: bool = False, threshold: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples from the Gumbel-Sigmoid distribution and optionally discretizes.\n",
    "    The discretization converts the values greater than `threshold` to 1 and the rest to 0.\n",
    "    The code is adapted from the official PyTorch implementation of gumbel_softmax:\n",
    "    https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "\n",
    "    Args:\n",
    "      logits: `[..., num_features]` unnormalized log probabilities\n",
    "      tau: non-negative scalar temperature\n",
    "      hard: if ``True``, the returned samples will be discretized,\n",
    "            but will be differentiated as if it is the soft sample in autograd\n",
    "     threshold: threshold for the discretization,\n",
    "                values greater than this will be set to 1 and the rest to 0\n",
    "\n",
    "    Returns:\n",
    "      Sampled tensor of same shape as `logits` from the Gumbel-Sigmoid distribution.\n",
    "      If ``hard=True``, the returned samples are discretized according to `threshold`, otherwise they will\n",
    "      be probability distributions.\n",
    "\n",
    "    \"\"\"\n",
    "    gumbels = (\n",
    "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "    )  # ~Gumbel(0, 1)\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)\n",
    "    y_soft = gumbels.sigmoid()\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        indices = (y_soft > threshold).nonzero(as_tuple=True)\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "        y_hard[indices[0], indices[1]] = 1.0\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "layer_size = 30\n",
    "input_embedding = 768\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 1\n",
    "thres = 0.7\n",
    "\n",
    "\n",
    "# define discriminator layers\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Linear(input_embedding, layer_size),\n",
    "    nn.Linear(layer_size, 1),\n",
    ")\n",
    "\n",
    "# compose feature gating model\n",
    "\n",
    "interpret_model = nn.Sequential(\n",
    "    discriminator,\n",
    "    model\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(interpret_model[0].parameters(), lr = learning_rate)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "interpret_model.to(device)\n",
    "raw_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "print('initialized model training')\n",
    "for epoch in range(num_epochs):\n",
    "    interpret_model.train()\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass token_type_ids to the model\n",
    "        outputs = raw_model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        output = interpret_model[0](embeddings)\n",
    "        g_mask = gumbel_sigmoid(output, tau=1, hard=True, threshold=thres).squeeze()\n",
    "\n",
    "        predictions = interpret_model[1](input_ids, attention_mask=g_mask)\n",
    "        y_pred = torch.argmax(predictions.logits, 1)\n",
    "        correct += torch.sum(y_pred == labels)\n",
    "        total += len(labels)\n",
    "\n",
    "        loss = criterion(predictions.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}, Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'interpretcc_text_sigmoid.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate InterpretCC Feature Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('interpretcc_text_sigmoid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model.eval()\n",
    "interpret_model.to(device)\n",
    "\n",
    "test_preds = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for i in test_df.index:\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('test sample ', str(count))\n",
    "        row = test_df.loc[i]\n",
    "        input_ids = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        labels = torch.Tensor([row['label']]).type(torch.LongTensor)\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        embeddings = raw_model(input_ids).last_hidden_state\n",
    "        output = interpret_model[0](embeddings)\n",
    "        g_mask = gumbel_sigmoid(output, tau=1, hard=True, threshold=thres).squeeze()\n",
    "\n",
    "        predictions = interpret_model[1](input_ids, attention_mask=g_mask)\n",
    "        test_preds.append(predictions)\n",
    "        predicted_labels = torch.argmax(predictions['logits'])\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbd",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
