{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch transformers datasets nltk jupyter ipywidgets sentence-transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To import the Transformer Models\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# to convert to Dataset datatype - the transformers library does not work well with pandas\n",
    "from datasets import Dataset\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset does not contain class labels, so we need to explicitly provide it\n",
    "data_path = '../data/ag-news-classification-dataset'\n",
    "train_df=pd.read_csv(os.path.join(data_path,'train.csv'),names=['label','Title','Description'])\n",
    "val_df=pd.read_csv(os.path.join(data_path,'test.csv'),names=['label','Title','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the 'title' and 'description' column\n",
    "train_df['text']=(train_df['Title']+train_df['Description'])\n",
    "train_df.drop(columns=['Title','Description'],axis=1,inplace=True)\n",
    "train_df.head()\n",
    "\n",
    "val_df['text']=(val_df['Title']+val_df['Description'])\n",
    "val_df.drop(columns=['Title','Description'],axis=1,inplace=True)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text=re.sub(r'[\\\\//-]',' ',text)\n",
    "    text=re.sub(r'[,.$#?;:\\'(){}!|0-9]',' ',text)\n",
    "    return text\n",
    "\n",
    "# the apply method applies a function along an axis of dataframe\n",
    "train_df['text']=train_df['text'].apply(remove_punctuations)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['text']=val_df['text'].apply(remove_punctuations)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    clean_text=[]\n",
    "    for word in text.split(' '):\n",
    "        if word not in english_stopwords:\n",
    "            clean_text.append(word)\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "# remove stopwords\n",
    "train_df['text']=train_df['text'].apply(remove_stopwords)\n",
    "\n",
    "# the class label in dataset contains labels as 1,2,3,4 but the model needs 0,1,2,3, so we subtract 1 from all\n",
    "train_df['label']=train_df['label'].apply(lambda x:x-1)\n",
    "\n",
    "# remove stopwords\n",
    "val_df['text']=val_df['text'].apply(remove_stopwords)\n",
    "\n",
    "# the class label in dataset contains labels as 1,2,3,4 but the model needs 0,1,2,3, so we subtract 1 from all\n",
    "val_df['label']=val_df['label'].apply(lambda x:x-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=train_test_split(train_df[['text','label']],train_size=.3,shuffle=True, random_state=0)\n",
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set has 36000 samples and testing set has 10000 samples for the purpose of a fast training loop\n",
    "test_df = test_df[:10000]\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router: Classify words into Dewey Decimal Code Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Dewey Decimal Code subcategorization\n",
    "\n",
    "with open('../data/ddc_subcategories.pkl', 'rb') as f:\n",
    "    ddc_subcategories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Sentence-BERT model\n",
    "sent_model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "def get_sent_embedding(text):\n",
    "    embedding = sent_model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "def get_word_embeddings_from_sent(sent):\n",
    "    words = sent.split(' ')\n",
    "    return list(map(get_sent_embedding, words))\n",
    "    \n",
    "# Flatten the list of subcategories and maintain a map to their main category\n",
    "flattened_subcategories = []\n",
    "category_map = {} \n",
    "for main_cat, subcats in ddc_subcategories.items():\n",
    "    for subcat in subcats:\n",
    "        flattened_subcategories.append(subcat)\n",
    "        category_map[subcat] = main_cat\n",
    "\n",
    "# Compute embeddings for each subcategory\n",
    "subcategory_embeddings = np.array([get_sent_embedding(subcat) for subcat in flattened_subcategories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example word sanity check\n",
    "word = \"school\"\n",
    "word_embedding = get_sent_embedding(word)\n",
    "print(\"word:\", word)\n",
    "\n",
    "# Calculate distances\n",
    "distances = cdist([word_embedding], subcategory_embeddings, metric='cosine').squeeze()\n",
    "\n",
    "# Find the closest category\n",
    "closest_idx = np.argmin(distances)\n",
    "closest_category = flattened_subcategories[closest_idx]\n",
    "closest_main_category = int(category_map[closest_category])/100\n",
    "print(closest_idx, closest_category, closest_main_category)\n",
    "\n",
    "sorted(zip(distances, flattened_subcategories))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"wesleyacheng/news-topic-classification-with-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"wesleyacheng/news-topic-classification-with-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.encoding = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.loc[idx]\n",
    "\n",
    "        # routing\n",
    "        word_embeddings = get_word_embeddings_from_sent(row['text'])\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = [cdist([w], subcategory_embeddings, metric='cosine').squeeze() for w in word_embeddings]\n",
    "        idxs = [np.argmin(d) for d in distances]\n",
    "        closest_categories = [flattened_subcategories[closest_idx] for closest_idx in idxs]\n",
    "        closest_main_categories = [int(int(category_map[closest_category])/100) for closest_category in closest_categories]\n",
    "        closest_main_categories = list(np.asarray(closest_main_categories) + 1)\n",
    "        routing = [0] + closest_main_categories + [0]*(self.max_length - 1 - len(closest_main_categories))\n",
    "\n",
    "        input_ids = tokenizer(row['text'], padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "        label = row['label']\n",
    "        routing = np.array(routing)\n",
    "        routing = routing.reshape(routing.shape[0], 1)\n",
    "        return row['text'], input_ids, routing, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the InterpretCC Gated Routing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "t = 0.5\n",
    "tau = 1\n",
    "b = 8\n",
    "l = 16\n",
    "num_of_subnetworks = 10\n",
    "num_classes = 4\n",
    "num_epochs = 5\n",
    "lr = 1e-5\n",
    "scheduler_flag = False\n",
    "ss = 10\n",
    "gamma = 0.5\n",
    "train_num = 36000\n",
    "test_num = 3000\n",
    "sentence_embedding_size = 768\n",
    "max_word_length = 512\n",
    "mul_thres = 0.1\n",
    "\n",
    "best_val_accuracy = 0\n",
    "val_count = 0\n",
    "\n",
    "experiment = \"interpretcc_gated_routing\"\n",
    "\n",
    "config = {\"t\": t, \"tau\": tau, \"batch\": b, \"layer\": l, \"architecture\": experiment, \n",
    "          \"num_of_subnetworks\": num_of_subnetworks, \"num_classes\": num_classes, \n",
    "          \"num_epochs\": num_epochs, \"lr\": lr, \"scheduler_flag\": scheduler_flag, \n",
    "          \"ss\": ss, \"gamma\": gamma, \"train_num\": train_num, \"test_num\": test_num, \n",
    "          \"sentence_embedding_size\": sentence_embedding_size, \n",
    "          \"max_word_length\": max_word_length}\n",
    "\n",
    "user = \"vinitra\"\n",
    "project = \"interpretcc\"\n",
    "display_name = experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sigmoid(logits: torch.Tensor, tau: float = 1, hard: bool = False, threshold: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples from the Gumbel-Sigmoid distribution and optionally discretizes.\n",
    "    The discretization converts the values greater than `threshold` to 1 and the rest to 0.\n",
    "    The code is adapted from the official PyTorch implementation of gumbel_softmax:\n",
    "    https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "\n",
    "    Args:\n",
    "      logits: `[..., num_features]` unnormalized log probabilities\n",
    "      tau: non-negative scalar temperature\n",
    "      hard: if ``True``, the returned samples will be discretized,\n",
    "            but will be differentiated as if it is the soft sample in autograd\n",
    "     threshold: threshold for the discretization,\n",
    "                values greater than this will be set to 1 and the rest to 0\n",
    "\n",
    "    Returns:\n",
    "      Sampled tensor of same shape as `logits` from the Gumbel-Sigmoid distribution.\n",
    "      If ``hard=True``, the returned samples are discretized according to `threshold`, otherwise they will\n",
    "      be probability distributions.\n",
    "\n",
    "    \"\"\"\n",
    "    gumbels = (\n",
    "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "    )  # ~Gumbel(0, 1)\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)\n",
    "    y_soft = gumbels.sigmoid()\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        indices = (y_soft > threshold).nonzero(as_tuple=True)\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "        y_hard[indices[0], indices[1]] = 1.0\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with wandb.init(entity=user, project=project, name=display_name, config=config, mode=\"online\") as run:\n",
    "\n",
    "    # Initializations of parameters\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    val_count = 0\n",
    "\n",
    "    dataset = CustomDataset(train_df[:train_num], tokenizer, max_length=max_word_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=b, shuffle=True)\n",
    "\n",
    "    test_dataset = CustomDataset(test_df[:test_num], tokenizer, max_length=max_word_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=b, shuffle=True)\n",
    "\n",
    "    # Define discriminator network\n",
    "    discriminator = nn.Sequential(\n",
    "        nn.Linear(sentence_embedding_size + max_word_length, l),\n",
    "        nn.Linear(l, num_of_subnetworks), # predict for each subnetwork at once\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "    # 10 copies of subnetworks\n",
    "    subnetworks = [copy.deepcopy(model) for i in np.arange(num_of_subnetworks)]\n",
    "\n",
    "    # assemble discriminators and subnetworks in module lists\n",
    "    interpret_models = nn.ModuleList([discriminator] + subnetworks)\n",
    "    # interpret_models = torch.load('26Jan_BestVal_36000Train')\n",
    "    [sub.to(device) for sub in interpret_models]\n",
    "\n",
    "    # Initialize optimizers\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(interpret_models.parameters(), lr=lr)\n",
    "\n",
    "    # Define a learning rate scheduler\n",
    "    if scheduler_flag:\n",
    "        scheduler = StepLR(optimizer, step_size=ss, gamma=gamma)\n",
    "\n",
    "    tracking = {'text': [], 'input_ids': [], 'routing': [], 'adaptive': [], 'attn_mask': [], 'labels': []}\n",
    "\n",
    "    print('initialized model training')\n",
    "    for epoch in range(num_epochs):\n",
    "        # track correct and total for accuracy\n",
    "        correct = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "        total = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "        interpret_models.train()\n",
    "\n",
    "        # for each batch\n",
    "        for batch in tqdm(data_loader):\n",
    "\n",
    "            raw_text, tokenizer_output_raw, routing, labels = batch\n",
    "            # send to GPU\n",
    "            labels = labels.type(torch.LongTensor).to(device)\n",
    "            routing, input_ids = routing.to(device), tokenizer_output_raw['input_ids'].to(device)\n",
    "            original_attn_mask = tokenizer_output_raw['attention_mask'].to(device)\n",
    "            tracking['text'].append(raw_text)\n",
    "            tracking['input_ids'].append(input_ids)\n",
    "            tracking['routing'].append(routing.squeeze())\n",
    "            tracking['labels'].append(labels)\n",
    "            b = len(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # for all subnetworks, mask each subnetwork's assignment\n",
    "            # batch x max_word_length x classes\n",
    "            route_mask = nn.functional.one_hot(routing.squeeze(), num_classes=num_of_subnetworks+1)[:,:,1:]\n",
    "\n",
    "            # create gumbel mask\n",
    "            embeddings = torch.Tensor(sent_model.encode(raw_text)).to(device)\n",
    "            concat_inputs_routing = torch.cat((routing.squeeze(), embeddings), 1)\n",
    "            output = interpret_models[0](concat_inputs_routing)\n",
    "            adaptive_mask_repeat = output.repeat_interleave(max_word_length,dim=0).reshape(b,max_word_length,num_of_subnetworks)\n",
    "            tracking['adaptive'].append(output)\n",
    "\n",
    "\n",
    "            # AND both masks together\n",
    "            attn_mask = torch.mul(adaptive_mask_repeat, route_mask) > mul_thres\n",
    "            attn_mask = attn_mask.type(torch.IntTensor).to(device)\n",
    "            tracking['attn_mask'].append(attn_mask)\n",
    "\n",
    "\n",
    "            subnet_predictions = []\n",
    "            for subnet in np.arange(num_of_subnetworks):\n",
    "                subnet_attn = attn_mask[:,:, subnet]\n",
    "                if sum(sum(subnet_attn)) != 0:\n",
    "                    predictions = interpret_models[subnet+1](input_ids.squeeze(), attention_mask=subnet_attn)\n",
    "                    subnet_predictions.append(predictions.logits)\n",
    "\n",
    "                    y_pred = torch.argmax(predictions.logits, 1)\n",
    "                    correct[subnet] += torch.sum(y_pred == labels)\n",
    "                    total[subnet] += len(labels)\n",
    "                else:\n",
    "                    subnet_predictions.append(torch.zeros((b, num_classes)).to(device))\n",
    "\n",
    "            g_weighting = output.t().repeat_interleave(b).reshape(num_of_subnetworks, b, b)[:,:,:num_classes]\n",
    "            subnet_predictions = torch.stack(subnet_predictions, dim=0)\n",
    "            weighted_predictions = torch.mul(g_weighting, subnet_predictions)\n",
    "            weighted_predictions = torch.sum(weighted_predictions, dim=0)\n",
    "\n",
    "            loss = criterion(weighted_predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            y_pred = torch.argmax(weighted_predictions, 1)\n",
    "            correct[num_of_subnetworks] += torch.sum(y_pred == labels)\n",
    "            total[num_of_subnetworks] += len(labels)\n",
    "\n",
    "            if scheduler_flag:\n",
    "                scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}, Train Accuracy: {correct[num_of_subnetworks]/total[num_of_subnetworks]}\")\n",
    "        [print(\"Subnet: \", subnet, \"| Train Accuracy:\", correct[subnet]/(total[subnet]+0.00001), \"| Total:\", total[subnet]) for subnet in np.arange(num_of_subnetworks)]\n",
    "        run.log({\"Epoch\": epoch + 1, \"Subnet\": -1, \"Loss\": loss.item(), \"Accuracy\": correct[num_of_subnetworks]/total[num_of_subnetworks]})\n",
    "        [run.log({\"Epoch\": epoch + 1, \"Subnet\": subnet, \"Train Accuracy\": correct[subnet]/(total[subnet]+0.00001), \"Train Total\": total[subnet]}) for subnet in np.arange(num_of_subnetworks)]\n",
    "        \n",
    "        torch.save(interpret_models, experiment)\n",
    "\n",
    "        with open('correct_' + experiment + '.pickle', 'wb') as file:\n",
    "            pickle.dump(correct, file)\n",
    "\n",
    "        with open('total_' + experiment + '.pickle', 'wb') as file:\n",
    "            pickle.dump(total, file)\n",
    "\n",
    "        with open('tracking_' + experiment + '.pickle', 'wb') as file:\n",
    "            pickle.dump(tracking, file)\n",
    "\n",
    "        interpret_models.eval()\n",
    "        test_dataset = CustomDataset(test_df[:test_num], tokenizer, max_length=max_word_length)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=b, shuffle=True)\n",
    "\n",
    "        with torch.no_grad():    \n",
    "            # track correct and total for accuracy\n",
    "            correct_test = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "            total_test = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "            tracking_test = {'text': [], 'input_ids': [], 'routing': [], 'adaptive': [], 'attn_mask': [], 'labels': []}\n",
    "\n",
    "            # for each batch\n",
    "            for batch in tqdm(test_loader):\n",
    "\n",
    "                raw_text, tokenizer_output_raw, routing, labels = batch\n",
    "                # send to GPU\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "                routing, input_ids = routing.to(device), tokenizer_output_raw['input_ids'].to(device)\n",
    "                original_attn_mask = tokenizer_output_raw['attention_mask'].to(device)\n",
    "                tracking_test['text'].append(raw_text)\n",
    "                tracking_test['input_ids'].append(input_ids)\n",
    "                tracking_test['routing'].append(routing.squeeze())\n",
    "                tracking_test['labels'].append(labels)\n",
    "                b = len(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # for all subnetworks, mask each subnetwork's assignment\n",
    "                # batch x max_word_length x classes\n",
    "                route_mask = nn.functional.one_hot(routing.squeeze(), num_classes=num_of_subnetworks+1)[:,:,1:]\n",
    "\n",
    "                # create gumbel mask\n",
    "                embeddings = torch.Tensor(sent_model.encode(raw_text)).to(device)\n",
    "                concat_inputs_routing = torch.cat((routing.squeeze(), embeddings), 1)\n",
    "                output = interpret_models[0](concat_inputs_routing)\n",
    "                gumbel_mask = torch.Tensor(gumbel_sigmoid(output, tau=1, hard=True, threshold=t).squeeze()).to(device)\n",
    "                adaptive_mask_repeat = gumbel_mask.repeat_interleave(max_word_length,dim=0).reshape(b,max_word_length,num_of_subnetworks)\n",
    "                tracking['adaptive'].append(gumbel_mask)\n",
    "\n",
    "\n",
    "                # AND both masks together\n",
    "                attn_mask = torch.mul(adaptive_mask_repeat, route_mask) > mul_thres\n",
    "                attn_mask = attn_mask.type(torch.IntTensor).to(device)\n",
    "                tracking['attn_mask'].append(attn_mask)\n",
    "\n",
    "\n",
    "                subnet_predictions = []\n",
    "                for subnet in np.arange(num_of_subnetworks):\n",
    "                    subnet_attn = attn_mask[:,:, subnet]\n",
    "                    if sum(sum(subnet_attn)) != 0:\n",
    "                        predictions = interpret_models[subnet+1](input_ids.squeeze(), attention_mask=subnet_attn)\n",
    "                        subnet_predictions.append(predictions.logits)\n",
    "\n",
    "                        y_pred = torch.argmax(predictions.logits, 1)\n",
    "                        correct_test[subnet] += torch.sum(y_pred == labels)\n",
    "                        total_test[subnet] += len(labels)\n",
    "\n",
    "                    else:\n",
    "                        subnet_predictions.append(torch.zeros((b, num_classes)).to(device))\n",
    "\n",
    "                g_weighting = gumbel_mask.t().repeat_interleave(b).reshape(num_of_subnetworks, b, b)[:,:,:num_classes]\n",
    "                subnet_predictions = torch.stack(subnet_predictions, dim=0)\n",
    "                weighted_predictions = torch.mul(g_weighting, subnet_predictions)\n",
    "                weighted_predictions = torch.sum(weighted_predictions, dim=0)\n",
    "\n",
    "                y_pred = torch.argmax(weighted_predictions, 1)\n",
    "                correct_test[num_of_subnetworks] += torch.sum(y_pred == labels)\n",
    "                total_test[num_of_subnetworks] += len(labels)\n",
    "\n",
    "            val_accuracy = correct_test[num_of_subnetworks]/(total_test[num_of_subnetworks])\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_model = interpret_models\n",
    "                torch.save(best_model, experiment)\n",
    "                best_val_accuracy = val_accuracy\n",
    "            else:\n",
    "                val_count += 1\n",
    "\n",
    "            print(\"Val Accuracy: \", correct_test[num_of_subnetworks]/(total_test[num_of_subnetworks]))    \n",
    "            [print(\"Subnet: \", subnet, \"| Val Accuracy:\", correct_test[subnet]/(total_test[subnet]+0.00001), \"| Val Total:\", total_test[subnet]) for subnet in np.arange(num_of_subnetworks)]\n",
    "            run.log({\"Epoch\": epoch + 1, \"Subnet\": -1, \"Loss\": loss.item(), \"Val Accuracy\": correct[num_of_subnetworks]/total[num_of_subnetworks]})\n",
    "            [run.log({\"Epoch\": epoch + 1, \"Subnet\": subnet, \"Val Accuracy\": correct[subnet]/(total[subnet]+0.00001), \"Val Total\": total[subnet]}) for subnet in np.arange(num_of_subnetworks)]\n",
    "            \n",
    "            if val_count > 1:\n",
    "                torch.save(best_model, experiment)\n",
    "                break\n",
    "\n",
    "torch.save(best_model, experiment)\n",
    "\n",
    "with open('correct_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(correct, file)\n",
    "\n",
    "with open('total_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(total, file)\n",
    "\n",
    "with open('tracking_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(tracking, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate InterpretCC Gated Routing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interpret = torch.load(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interpret.eval()\n",
    "new_interpret.to(device)\n",
    "\n",
    "test_dataset = CustomDataset(test_df[test_num:test_num*2].reset_index(), tokenizer, max_length=max_word_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=b, shuffle=True)\n",
    "\n",
    "with torch.no_grad():    \n",
    "    # track correct and total for accuracy\n",
    "    correct_test = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "    total_test = {k:0 for k in np.arange(num_of_subnetworks + 1)}\n",
    "    tracking_test = {'text': [], 'input_ids': [], 'routing': [], 'adaptive': [], 'attn_mask': [], 'labels': []}\n",
    "\n",
    "    # for each batch\n",
    "    for batch in tqdm(test_loader):\n",
    "\n",
    "        raw_text, tokenizer_output_raw, routing, labels = batch\n",
    "        # send to GPU\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        routing, input_ids = routing.to(device), tokenizer_output_raw['input_ids'].to(device)\n",
    "        original_attn_mask = tokenizer_output_raw['attention_mask'].to(device)\n",
    "        tracking_test['text'].append(raw_text)\n",
    "        tracking_test['input_ids'].append(input_ids)\n",
    "        tracking_test['routing'].append(routing.squeeze())\n",
    "        tracking_test['labels'].append(labels)\n",
    "        b = len(labels)\n",
    "\n",
    "        # for all subnetworks, mask each subnetwork's assignment\n",
    "        # batch x max_word_length x classes\n",
    "        route_mask = nn.functional.one_hot(routing.squeeze(), num_classes=num_of_subnetworks+1)[:,:,1:]\n",
    "\n",
    "        # create gumbel mask\n",
    "        embeddings = torch.Tensor(sent_model.encode(raw_text)).to(device)\n",
    "        concat_inputs_routing = torch.cat((routing.squeeze(), embeddings), 1)\n",
    "        output = new_interpret[0](concat_inputs_routing)\n",
    "        gumbel_mask = torch.Tensor(gumbel_sigmoid(output, tau=1, hard=True, threshold=t).squeeze()).to(device)\n",
    "        adaptive_mask_repeat = gumbel_mask.repeat_interleave(max_word_length,dim=0).reshape(b,max_word_length,num_of_subnetworks)\n",
    "        tracking_test['adaptive'].append(gumbel_mask)\n",
    "\n",
    "\n",
    "        # AND both masks together\n",
    "        attn_mask = torch.mul(adaptive_mask_repeat, route_mask) > mul_thres\n",
    "        attn_mask = attn_mask.type(torch.IntTensor).to(device)\n",
    "        tracking_test['attn_mask'].append(attn_mask)\n",
    "\n",
    "\n",
    "        subnet_predictions = []\n",
    "        for subnet in np.arange(num_of_subnetworks):\n",
    "            subnet_attn = attn_mask[:,:, subnet]\n",
    "            if sum(sum(subnet_attn)) != 0:\n",
    "                predictions = new_interpret[subnet+1](input_ids.squeeze(), attention_mask=subnet_attn)\n",
    "                subnet_predictions.append(predictions.logits)\n",
    "\n",
    "                y_pred = torch.argmax(predictions.logits, 1)\n",
    "                correct_test[subnet] += torch.sum(y_pred == labels)\n",
    "                total_test[subnet] += len(labels)\n",
    "\n",
    "            else:\n",
    "                subnet_predictions.append(torch.zeros((b, num_classes)).to(device))\n",
    "\n",
    "        g_weighting = output.t().repeat_interleave(b).reshape(num_of_subnetworks, b, b)[:,:,:num_classes]\n",
    "        subnet_predictions = torch.stack(subnet_predictions, dim=0)\n",
    "        weighted_predictions = torch.mul(g_weighting, subnet_predictions)\n",
    "        weighted_predictions = torch.sum(weighted_predictions, dim=0)\n",
    "\n",
    "        y_pred = torch.argmax(weighted_predictions, 1)\n",
    "        correct_test[num_of_subnetworks] += torch.sum(y_pred == labels)\n",
    "        total_test[num_of_subnetworks] += len(labels)\n",
    "    \n",
    "    print(\"Test Accuracy: \", correct_test[num_of_subnetworks]/(total_test[num_of_subnetworks]))    \n",
    "    [print(\"Subnet: \", subnet, \"| Test Accuracy:\", correct_test[subnet]/(total_test[subnet]+0.00001), \"| Test Total:\", total_test[subnet]) for subnet in np.arange(num_of_subnetworks)]\n",
    "\n",
    "\n",
    "with open('correct_test_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(correct_test, file)\n",
    "    \n",
    "with open('total_test_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(total_test, file)\n",
    "    \n",
    "with open('tracking_test_' + experiment + '.pickle', 'wb') as file:\n",
    "    pickle.dump(tracking_test, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbd",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
